---
title: "R Notebook"
output: html_notebook
---

# Introduction

This analysis attempts to predicate the probability for survival of the Titanic passengers. In order to do this, I will use the different features available about the passengers, use a subset of the data to train an algorithm and then run the algorithm on the rest of the data set to get a prediction. 

In this analysis I asked the following questions:

1. What is the relationship the features and a passengerâ€™s chance of survival.

2. Prediction of survival for the entire ship.


# Data loading and cleaning

```{r echo=TRUE, message=FALSE, warning=FALSE}
# I used the following packages for this analysis:

library(ggplot2)
library(dplyr)
library(GGally)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(mltools)
library(e1071)

# Data Loading:

test <- read.csv('./data/test.csv', stringsAsFactors = FALSE)
train <- read.csv('./data/train.csv', stringsAsFactors = FALSE)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Creating a new data set with both the test and the train sets
full <- bind_rows(train,test)
LT=dim(train)[1]
# Checking the structure
str(full)
```

```{r}
View(full)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Missing values
colSums(is.na(full))
colSums(full=="")
```

```{r}
# Check columns with NAs
full[, colSums(is.na(full)) == 0]

```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# We have a lot of missing data in the Age feature (263/1309)

# Let's change the empty strings in Embarked to the first choice "C"
full$Embarked[full$Embarked==""]="C"

# Let's see how many features we can move to factors
apply(full,2, function(x) length(unique(x)))

# Let's move the features Survived, Pclass, Sex, Embarked to be factors
cols<-c("Survived","Pclass","Sex","Embarked")
for (i in cols){
  full[,i] <- as.factor(full[,i])
}

# Now lets look on the structure of the full data set
str(full)
```
# Analysis

Look at the relationships between the different features:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# First, let's look at the relationship between sex and survival:
ggplot(data=full[1:LT,],aes(x=Sex,fill=Survived))+geom_bar()

```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Survival as a function of Embarked:
ggplot(data = full[1:LT,],aes(x=Embarked,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
t<-table(full[1:LT,]$Embarked,full[1:LT,]$Survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
t
#It looks that you have a better chance to survive if you Embarked in 'C' (55% compared to 33% and 38%).

# Survival as a function of Pclass:
ggplot(data = full[1:LT,],aes(x=Pclass,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# It looks like you have a better chance to survive if you in lower ticket class.
# Now, let's divide the graph of Embarked by Pclass:
ggplot(data = full[1:LT,],aes(x=Embarked,fill=Survived))+geom_bar(position="fill")+facet_wrap(~Pclass)
```

# Now it's not so clear that there is a correlation between Embarked and Survival. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Survivial as a function of SibSp and Parch
ggplot(data = full[1:LT,],aes(x=SibSp,fill=Survived))+geom_bar()
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = full[1:LT,],aes(x=Parch,fill=Survived))+geom_bar()
```


# The dymanics of SibSp and Parch are very close one each other.
# Let's try to look at another parameter: family size.
```{r echo=TRUE, message=FALSE, warning=FALSE}
full$FamilySize <- full$SibSp + full$Parch +1;
full1<-full[1:LT,]
ggplot(data = full1[!is.na(full[1:LT,]$FamilySize),],aes(x=FamilySize,fill=Survived))+geom_histogram(binwidth =1,position="fill")+ylab("Frequency")
# That shows that families with a family size bigger or equal to 2 but less than 6 have a more than 50% to survive, in contrast to families with 1 member or more than 5 members. 
```


# Survival as a function of age:
```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = full1[!(is.na(full[1:LT,]$Age)),],aes(x=Age,fill=Survived))+geom_histogram(binwidth =3)
ggplot(data = full1[!is.na(full[1:LT,]$Age),],aes(x=Age,fill=Survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frequency")
```

# Children (younger than 15YO) and old people (80 and up) had a better chance to survive.

# Is there a correlation between Fare and Survival?

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = full[1:LT,], aes(x=Fare,fill=Survived)) + geom_histogram(binwidth =20, position="fill")
full$Fare[is.na(full$Fare)] <- mean(full$Fare,na.rm=T)

```

# It seems like if your fare is bigger, than you have a better chance to survive.

```{r echo=TRUE, message=FALSE, warning=FALSE}
sum(is.na(full$Age))
```


# There are a lot of missing values in the Age feature, so I'll put the mean instead of the missing values.
```{r echo=TRUE, message=FALSE, warning=FALSE}
full$Age[is.na(full$Age)] <- mean(full$Age,na.rm=T)
sum(is.na(full$Age))
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# The title of the passenger can affect his survive:
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)
full$Title[full$Title == 'Mlle']<- 'Miss' 
full$Title[full$Title == 'Ms']<- 'Miss'
full$Title[full$Title == 'Mme']<- 'Mrs' 
full$Title[full$Title == 'Lady']<- 'Miss'
full$Title[full$Title == 'Dona']<- 'Miss'
officer<- c('Capt','Col','Don','Dr','Jonkheer','Major','Rev','Sir','the Countess')
full$Title[full$Title %in% officer]<-'Officer'

full$Title<- as.factor(full$Title)

ggplot(data = full[1:LT,],aes(x=Title,fill=Survived))+geom_bar(position="fill")+ylab("Frequency")

```


# Prediction

At this time point we want to predict the chance of survival as a function of the other features. I'm going to keep just the correlated features: Pclass, Sex, Age, SibSp, Parch, Title and Fare.
I'm going to divide the train set into two sets: training set (train.set) and test set (test.set) to be able to estimate the error of the prediction.

# Logistic Regression


```{r echo=TRUE,message=FALSE,warning=FALSE}
# The train set with the important features 
train_im<- full[1:LT,c("Survived","Pclass","Sex","Age","Fare","SibSp","Parch","Title")]

ind<-sample(1:dim(train_im)[1],500) # Sample of 500 out of 891
train.set<-train_im[ind,] # The train set of the model
test.set<-train_im[-ind,] # The test set of the model

# Let's try to run a logistic regression
model <- glm(Survived ~.,family=binomial(link='logit'),data=train.set)
summary(model)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
# We can see that SibSp, Parch and Fare are not statisticaly significant. 
# Let's look at the prediction of this model on the test set (test.set):
pred.train <- predict(model,test.set)
pred.train <- ifelse(pred.train > 0.5,1,0)
# Mean of the true prediction 
mean(pred.train==test.set$Survived)
```


```{r echo=TRUE, message=FALSE, warning=FALSE}
t1<-table(pred.train,test.set$Survived)
# Presicion and recall of the model
presicion<- t1[1,1]/(sum(t1[1,]))
recall<- t1[1,1]/(sum(t1[,1]))
presicion
recall
# F1 score
F1<- 2*presicion*recall/(presicion+recall)
F1
```
# F1 score on the initial test set is over 0.86. This pretty good.

# Let's run it on the test set:
```{r echo=TRUE, message=FALSE, warning=FALSE}

test_im<-full[LT+1:1309,c("Pclass","Sex","Age","SibSp","Parch","Fare","Title")]

pred.test <- predict(model,test_im)[1:418]
pred.test <- ifelse(pred.test > 0.5,1,0)
res<- data.frame(test$PassengerId,pred.test)
names(res)<-c("PassengerId","Survived")
write.csv(res,file="res.csv",row.names = F)
```

# KNN

```{r}
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
fit.knn <- train(Survived~., data=train.set, method="knn", metric="Accuracy", trControl=trainControl)
fit.knn
```


# SVM

```{r}
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
fit.knn <- train(Survived~., data=train.set, method="svmLinear", metric="Accuracy", trControl=trainControl)
fit.knn
```

# Decision Trees


```{r echo=TRUE,warning=FALSE,message=FALSE}
model_dt<- rpart(Survived ~.,data=train.set, method="class")
rpart.plot(model_dt)

pred.train.dt <- predict(model_dt,test.set,type = "class")
mean(pred.train.dt==test.set$Survived)
t2<-table(pred.train.dt,test.set$Survived)

presicion_dt<- t2[1,1]/(sum(t2[1,]))
recall_dt<- t2[1,1]/(sum(t2[,1]))
presicion_dt
recall_dt
F1_dt<- 2*presicion_dt*recall_dt/(presicion_dt+recall_dt)
F1_dt
# Let's run this model on the test set:
pred.test.dt <- predict(model_dt,test_im,type="class")[1:418]
res_dt<- data.frame(test$PassengerId,pred.test.dt)
names(res_dt)<-c("PassengerId","Survived")
write.csv(res_dt,file="res_dt.csv",row.names = F)
```

# Random Forest

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Let's try to predict survival using a random forest.
model_rf<-randomForest(x=train.set[, 2:8],
               y=train.set$Survived,
               importance=TRUE
               )
plot(model_rf)
```

```{r}
# Let's look at the error
pred.train.rf <- predict(model_rf,test.set)
sprintf("Accuracy: %f", (mean(pred.train.rf==test.set$Survived)))
t1<-table(pred.train.rf,test.set$Survived)
prec<- t1[1,1]/(sum(t1[1,]))
recall<- t1[1,1]/(sum(t1[,1]))
F1<- 2*presicion*recall/(presicion+recall)

sprintf("Presicion: %f", prec)
sprintf("Recall: %f", recall[1])
sprintf("F1 score: %f", F1)
```


```{r}
probas <- predict(model_rf, test.set, type="prob")
probas[1:5,1:2]
```


```{r}
set.seed(123) 
train.control <- trainControl(method = "cv",
                              number = 5,
                              classProbs=TRUE, summaryFunction = twoClassSummary)
rfcv <- train(make.names(Survived)~ ., data=train.set, trControl=train.control, method='rf', metric='ROC')
print(rfcv)
```

```{r}

ctrl <- trainControl(classProbs = TRUE,
                     summaryFunction = twoClassSummary)

set.seed(29510)
lda_data <-
  learning_curve_dat(dat = train.set,
                     outcome = "Survived",
                     test_prop = 1/4,
                     ## `train` arguments:
                     method = "rf",
                     metric = "ROC",
                     trControl = ctrl)



ggplot(lda_data, aes(x = Training_Size, y = ROC, color = Data)) +
  geom_smooth(method = loess, span = .8) +
  theme_bw()
```

```{r}
set.seed(123) 
train.control <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 3,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "random")
# search: {random, grid}
# tuneLength: number of combinations
rfhyp <- train(make.names(Survived)~ ., data=train.set,
              trControl=train.control,
              method='rf',
              metric='ROC',
              tuneLength=10
              )
print(rfhyp)
```


```{r}
plot(rfhyp)
```

```{r}
library(pROC)
preds <- predict(rfhyp, test.set, type="prob")
roc_curve <- roc(test.set$Survived, preds$X1)
#plot.roc(roc_curve, print.thres="best", print.thres.best.method = "closest.topleft")


ggroc(roc_curve, alpha = 1, colour = "orange", linetype = 1, size = 1) + theme_minimal() + ggtitle(sprintf("ROC curve. AUC = %.3f",roc_curve$auc)) + 
    geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed") + xlab("FPR") + ylab("TPR") 


```

